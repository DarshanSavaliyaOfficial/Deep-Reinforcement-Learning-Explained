{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DRL_09_VI_Algorithm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suvKdcFqXcd5",
        "colab_type": "text"
      },
      "source": [
        "DEEP REINFORCEMENT LEARNING EXPLAINED - 09\n",
        "# **The Value Iteration Algorithm**\n",
        "## Value Iteration in Practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLiVqPvwmCJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import collections\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "#ENV_NAME = \"FrozenLake8x8-v0\"  \n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N =100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxTWfToPrjxe",
        "colab_type": "text"
      },
      "source": [
        "### The Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgnZfMGvk9Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(\n",
        "            collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            if is_done:\n",
        "                self.state = self.env.reset() \n",
        "            else: \n",
        "                self.state = new_state\n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        target_counts = self.transits[(state, action)]\n",
        "        total = sum(target_counts.values())\n",
        "        action_value = 0.0\n",
        "        for tgt_state, count in target_counts.items():\n",
        "            reward = self.rewards[(state, action, tgt_state)]\n",
        "            val = reward + GAMMA * self.values[tgt_state]\n",
        "            action_value += (count / total) * val\n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [\n",
        "                self.calc_action_value(state, action)\n",
        "                for action in range(self.env.action_space.n)\n",
        "            ]\n",
        "            self.values[state] = max(state_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scTw9juUrf3u",
        "colab_type": "text"
      },
      "source": [
        "### Train the Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcUs5zNa7vUI",
        "colab_type": "code",
        "outputId": "016cd7a8-3309-4123-98d9-811224ad057c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "test_env = gym.make(ENV_NAME)\n",
        "agent = Agent()\n",
        "writer = SummaryWriter()\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        " \n",
        "while best_reward < REWARD_GOAL:\n",
        "        \n",
        "        agent.play_n_random_steps(N)\n",
        "\n",
        "        agent.value_iteration()\n",
        "\n",
        "        iter_no += 1\n",
        "        reward_test = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            total_reward = 0.0\n",
        "            state = test_env.reset()\n",
        "            while True:\n",
        "                action = agent.select_action(state)\n",
        "                new_state, new_reward, is_done, _ = test_env.step(action)\n",
        "                total_reward += new_reward\n",
        "                if is_done: break\n",
        "                state = new_state\n",
        "            reward_test += total_reward\n",
        "        reward_test /= TEST_EPISODES\n",
        "\n",
        "        writer.add_scalar(\"reward\", reward_test, iter_no)\n",
        "        if reward_test > best_reward:\n",
        "            print(\"Best reward updated %.2f at iteration %d \" % (reward_test ,iter_no) )\n",
        "            best_reward = reward_test\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.10 at iteration 12 \n",
            "Best reward updated 0.35 at iteration 14 \n",
            "Best reward updated 0.60 at iteration 15 \n",
            "Best reward updated 0.70 at iteration 24 \n",
            "Best reward updated 0.75 at iteration 35 \n",
            "Best reward updated 0.95 at iteration 36 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPT9RVqaL93f",
        "colab_type": "text"
      },
      "source": [
        "### Test the Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPjUCudxM8S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_test_env = gym.make(ENV_NAME) \n",
        "state = new_test_env.reset()\n",
        "new_test_env.render()\n",
        "is_done = False\n",
        "iter_no = 0\n",
        "while not is_done:\n",
        "     print (state)\n",
        "     action = agent.select_action(state)\n",
        "     new_state, reward, is_done, _ = new_test_env.step(action)\n",
        "     test_env.render()\n",
        "     state = new_state\n",
        "     iter_no +=1\n",
        "print(\"reward =    \", reward)\n",
        "print(\"iterations =\", iter_no)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymICwXwAkyuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmobnomGlHBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensorboard  --logdir=runs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}